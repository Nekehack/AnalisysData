{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc923c00-54af-4ec9-8b56-3ff95834344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3d1785ef-3724-402e-8dea-f5ed33f1bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c67c5de6-d5f1-411e-85f1-71bcb94b2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Удаление технических логов и шума\n",
    "    text = re.sub(r'Heap \\d+ bytes reserved', '', text)\n",
    "    text = re.sub(r'Timestamp : \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', '', text)\n",
    "    text = re.sub(r'<newline>', '\\n', text)  # Замена тегов на переносы строк\n",
    "    text = re.sub(r'\\s+', ' ', text)         # Удаление лишних пробелов\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1b570c6c-322d-463e-8dc1-45443c8daa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_stories(raw_text):\n",
    "    # Разделение на истории по разделителю \"...\"\n",
    "    stories = [preprocess_text(part) for part in raw_text.split(\"...\") if part.strip()]\n",
    "    return [story for story in stories if len(story.split()) > 50]  # Фильтрация слишком коротких текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "448b3b2e-ec30-4009-b688-c4ee9a1417a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_and_backstory(text):\n",
    "    # Разделение на финал (последние 2 предложения) и предысторию\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]\n",
    "    if len(sentences) < 3:\n",
    "        return None, None\n",
    "    final = '. '.join(sentences[-2:])\n",
    "    backstory = '. '.join(sentences[:-2])\n",
    "    return final, backstory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d2223f0-568a-4a79-9edc-ddaea18bea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingPromptsDataset(Dataset):\n",
    "    def __init__(self, stories, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        \n",
    "        # Извлечение пар \"финал-предыстория\"\n",
    "        for story in stories:\n",
    "            final, backstory = extract_final_and_backstory(story)\n",
    "            if final and backstory:\n",
    "                self.data.append((final, backstory))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        final, backstory = self.data[idx]\n",
    "        \n",
    "        # Токенизация\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            f\"generate_prehistory: {final}\",\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            backstory,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a9e8dd0f-4e56-4a4f-acea-0232d5cddd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(csv_file, model_name='t5-small', epochs=1, batch_size=32, max_len=256):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    device = torch.device('cpu')  # Вместо 'cuda'\n",
    "    model.to(device)\n",
    "    \n",
    "    with open(csv_file, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    stories = split_into_stories(raw_text)[:1000]  # Используем только 1000 примеров\n",
    "    \n",
    "    dataset = WritingPromptsDataset(stories, tokenizer, max_len=max_len)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device)\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Проверка весов модели...\")\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        \n",
    "        model.save_pretrained('wp_reverse_model')\n",
    "        tokenizer.save_pretrained('wp_reverse_model')\n",
    "        print(\"Модель успешно сохранена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении модели: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d18dee9-984a-44d1-9d37-285fa72deae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prehistory(final_part, model_path='wp_reverse_model'):\n",
    "    print(\"Загрузка модели...\")\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_path).cpu()\n",
    "        print(\"Модель загружена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке модели: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not final_part.strip():\n",
    "        raise ValueError(\"Входное предложение пустое!\")\n",
    "\n",
    "    print(\"Токенизация входных данных...\")\n",
    "    inputs = tokenizer(\n",
    "        f\"generate_prehistory: {final_part}\",\n",
    "        return_tensors='pt'\n",
    "    ).to('cpu')\n",
    "\n",
    "    try:\n",
    "        print(\"Генерация текста...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=150,  # Уменьшено для ускорения\n",
    "            num_beams=3,     # Уменьшено для ускорения\n",
    "            early_stopping=True\n",
    "        )\n",
    "        print(\"Генерация завершена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при генерации: {e}\")\n",
    "        return None\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd74425-64e5-45cc-bc7f-07fdf0be3de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eebfb138-d937-41ed-8fd8-c242431bdf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████████████████████████████| 32/32 [12:20<00:00, 23.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.3886\n",
      "Проверка весов модели...\n",
      "shared.weight torch.Size([32128, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 8])\n",
      "encoder.block.0.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.0.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.1.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.2.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.3.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.4.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.5.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.final_layer_norm.weight torch.Size([512])\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 8])\n",
      "decoder.block.0.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.0.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.1.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.2.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.3.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.4.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.5.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.final_layer_norm.weight torch.Size([512])\n",
      "Модель успешно сохранена.\n",
      "Загрузка модели...\n",
      "Модель загружена.\n",
      "Токенизация входных данных...\n",
      "Генерация текста...\n",
      "Генерация завершена.\n",
      "Сгенерированная предыстория: \n"
     ]
    }
   ],
   "source": [
    "train_model('valid.txt', epochs=1)\n",
    "    \n",
    "# Генерация предыстории\n",
    "final = \"Clancy slits Rob's throat and returns to HQ.\"\n",
    "print(\"Сгенерированная предыстория:\", generate_prehistory(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81e98e95-24b3-426d-a268-ced2874124bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwp_reverse_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwp_reverse_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМодель сохранена!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cc3150-f8d8-4297-806b-c8a51629eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f10359dd-338f-47bb-bd7c-b38c5905371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load...\n",
      "Load finaly\n",
      "Сгенерированная предыстория: \n",
      "Load...\n",
      "Load finaly\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "final = \"Clancy slits Rob's throat and returns to HQ.\"\n",
    "print(\"Сгенерированная предыстория:\", generate_prehistory(final))\n",
    "a = generate_prehistory(final)\n",
    "with open('text1.txt', 'w') as file:\n",
    "    file.write(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ecbd80e-5498-4fe5-b939-f85ce69f7141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_warning()\n",
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f4985e2-fe68-4689-b318-84fbfd0917cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m-\u001b[39mh\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bade7f-dd48-4bdb-9b63-992f441943d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
