{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76bee398-565b-414a-a2ca-daced33213d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58b62015-4a81-4cea-91ab-4145d5f59f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, finals, backstories, tokenizer, max_len=128):\n",
    "        self.final = finals\n",
    "        self.backstory = backstories\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        final = self.final[idx]\n",
    "        backstory = self.backstory[idx]\n",
    "\n",
    "        # Токенизация\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            f\"predict_backstory: {final}\",\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            backstory,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "843805f4-afc0-4e5a-97ef-1ae1ea7cb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file):\n",
    "    df = pd.read_csv(csv_file, sep=';')\n",
    "    finals = df['Final'].values\n",
    "    backstories = df['Backstory'].values\n",
    "    print('finals: ', finals)\n",
    "    print(\"Образец данных:\")\n",
    "    for i, row in df.iterrows():\n",
    "        print(f\"Final: {row['Final']}\")\n",
    "        print(f\"Backstory: {row['Backstory']}\")\n",
    "        if i > 5:\n",
    "            break\n",
    "    \n",
    "    return finals, backstories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e80134c2-b21f-48c7-92c6-248e10ecb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(csv_file, model_name='t5-small', epochs=3, batch_size=8, max_len=128):\n",
    "    # Загрузка данных\n",
    "    finals, backstories = load_data(csv_file)\n",
    "    train_finals, val_finals, train_backstories, val_backstories = train_test_split(\n",
    "        finals, backstories, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    # Инициализация токенизатора и модели\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    inputs = tokenizer(\n",
    "        f\"predict_backstory: {final}\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Создание датасетов и загрузчиков\n",
    "    train_dataset = StoryDataset(train_finals, train_backstories, tokenizer, max_len)\n",
    "    val_dataset = StoryDataset(val_finals, val_backstories, tokenizer, max_len)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Оптимизатор\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    # Цикл обучения\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # inputs = {\n",
    "            #     'input_ids': batch['input_ids'].to(device),\n",
    "            #     'attention_mask': batch['attention_mask'].to(device),\n",
    "            #     'labels': batch['labels'].to(device)\n",
    "            # }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(device),\n",
    "                    'attention_mask': batch['attention_mask'].to(device),\n",
    "                    'labels': batch['labels'].to(device)\n",
    "                }\n",
    "                outputs = model(**inputs)\n",
    "                val_loss += outputs.loss.item()\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    # Сохранение модели\n",
    "    model.save_pretrained('story_generator_model')\n",
    "    tokenizer.save_pretrained('story_generator_model')\n",
    "    print(\"Модель успешно сохранена.\")\n",
    "    \n",
    "    print(\"Проверка весов модели...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14cb4bac-8ce9-484b-a7ec-0a7ae0e8a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Генерация текста\n",
    "def generate_backstory(final, model_path='story_generator_model'):\n",
    "    # Загрузка модели и токенизатора\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path).to('cpu')\n",
    "\n",
    "    # Токенизация входных данных\n",
    "    inputs = tokenizer(\n",
    "        f\"predict_backstory: {final}\",\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to('cpu')\n",
    "\n",
    "    # Генерация текста\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Декодирование текста\n",
    "    decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Отладочные выводы\n",
    "    print(\"Input IDs:\", inputs['input_ids'])\n",
    "    print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "    print(\"Generated output IDs:\", outputs)\n",
    "    print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "449605d4-7419-4b46-94ea-d8391f456d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finals:  ['He returned home and smiled.' 'She dreamed of adventure.'\n",
      " 'They celebrated their victory.' 'The sun set behind the mountains.'\n",
      " 'She hugged him tightly.' 'The door slammed shut.'\n",
      " 'He woke up in a cold sweat.' 'The crowd cheered loudly.'\n",
      " 'The letter brought tears to her eyes.' 'The storm passed, leaving calm.']\n",
      "Образец данных:\n",
      "Final: He returned home and smiled.\n",
      "Backstory: He went to the store. He bought milk. He met an old friend on the way.\n",
      "Final: She dreamed of adventure.\n",
      "Backstory: She read a book. She fell asleep.\n",
      "Final: They celebrated their victory.\n",
      "Backstory: They trained hard for months. They won the championship game.\n",
      "Final: The sun set behind the mountains.\n",
      "Backstory: They hiked all day. They reached the summit just in time.\n",
      "Final: She hugged him tightly.\n",
      "Backstory: He surprised her with flowers. She hadn’t seen him in years.\n",
      "Final: The door slammed shut.\n",
      "Backstory: She heard footsteps behind her. She ran as fast as she could.\n",
      "Final: He woke up in a cold sweat.\n",
      "Backstory: He had a nightmare about falling. He couldn’t shake the feeling.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstory.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Генерация текста\u001b[39;00m\n\u001b[1;32m      4\u001b[0m final \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHe returned home and smiled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[101], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(csv_file, model_name, epochs, batch_size, max_len)\u001b[0m\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# inputs = {\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#     'input_ids': batch['input_ids'].to(device),\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#     'attention_mask': batch['attention_mask'].to(device),\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#     'labels': batch['labels'].to(device)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     43\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1905\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1905\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1906\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1907\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1908\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1909\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1910\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1911\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1912\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1913\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1914\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1915\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1916\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1917\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1918\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1919\u001b[0m )\n\u001b[1;32m   1921\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:997\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    996\u001b[0m     err_msg_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minput_ids or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "train_model('story.csv', epochs=25, batch_size=10)\n",
    "# Генерация текста\n",
    "final = \"He returned home and smiled.\"\n",
    "backstory = generate_backstory(final)\n",
    "print(\"Сгенерированная основа истории:\", backstory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01fe76-10dd-427e-ba02-f98be1042c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c887d-0ce2-4324-b35a-e54beb4ad880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
