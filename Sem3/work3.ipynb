{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc923c00-54af-4ec9-8b56-3ff95834344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1785ef-3724-402e-8dea-f5ed33f1bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67c5de6-d5f1-411e-85f1-71bcb94b2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Удаление технических логов и шума\n",
    "    text = re.sub(r'Heap \\d+ bytes reserved', '', text)\n",
    "    text = re.sub(r'Timestamp : \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', '', text)\n",
    "    text = re.sub(r'<newline>', '\\n', text)  # Замена тегов на переносы строк\n",
    "    text = re.sub(r'\\s+', ' ', text)         # Удаление лишних пробелов\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b570c6c-322d-463e-8dc1-45443c8daa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_stories(raw_text):\n",
    "    # Разделение на истории по разделителю \"...\"\n",
    "    stories = [preprocess_text(part) for part in raw_text.split(\"...\") if part.strip()]\n",
    "    return [story for story in stories if len(story.split()) > 50]  # Фильтрация слишком коротких текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448b3b2e-ec30-4009-b688-c4ee9a1417a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_and_backstory(text):\n",
    "    # Разделение на финал (последние 2 предложения) и предысторию\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]\n",
    "    if len(sentences) < 3:\n",
    "        return None, None\n",
    "    final = '. '.join(sentences[-2:])\n",
    "    backstory = '. '.join(sentences[:-2])\n",
    "    return final, backstory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2223f0-568a-4a79-9edc-ddaea18bea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingPromptsDataset(Dataset):\n",
    "    def __init__(self, stories, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        \n",
    "        # Извлечение пар \"финал-предыстория\"\n",
    "        for story in stories:\n",
    "            final, backstory = extract_final_and_backstory(story)\n",
    "            if final and backstory:\n",
    "                self.data.append((final, backstory))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        final, backstory = self.data[idx]\n",
    "        \n",
    "        # Токенизация\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            f\"generate_prehistory: {final}\",\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        targets = self.tokenizer.encode_plus(\n",
    "            backstory,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9e8dd0f-4e56-4a4f-acea-0232d5cddd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(csv_file, model_name='t5-small', epochs=1, batch_size=32, max_len=256):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    device = torch.device('cpu')  # Вместо 'cuda'\n",
    "    model.to(device)\n",
    "    \n",
    "    with open('story.csv', 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    stories = split_into_stories(raw_text)[:1000]  # Используем только 1000 примеров\n",
    "    \n",
    "    dataset = WritingPromptsDataset(stories, tokenizer, max_len=max_len)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device)\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Проверка весов модели...\")\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        \n",
    "        model.save_pretrained('wp_reverse_model')\n",
    "        tokenizer.save_pretrained('wp_reverse_model')\n",
    "        print(\"Модель успешно сохранена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении модели: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d18dee9-984a-44d1-9d37-285fa72deae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prehistory(final_part, model_path='wp_reverse_model'):\n",
    "    print(\"Загрузка модели...\")\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_path).cpu()\n",
    "        print(\"Модель загружена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке модели: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not final_part.strip():\n",
    "        raise ValueError(\"Входное предложение пустое!\")\n",
    "\n",
    "    print(\"Токенизация входных данных...\")\n",
    "    inputs = tokenizer(\n",
    "        f\"generate_prehistory: {final_part}\",\n",
    "        return_tensors='pt'\n",
    "    ).to('cpu')\n",
    "\n",
    "    try:\n",
    "        print(\"Генерация текста...\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=150,  # Уменьшено для ускорения\n",
    "            num_beams=3,     # Уменьшено для ускорения\n",
    "            early_stopping=True\n",
    "        )\n",
    "        print(\"Generated output IDs:\", outputs)\n",
    "        print(\"Decoded text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        print(\"Генерация завершена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при генерации: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Проверка весов модели...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.shape)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eebfb138-d937-41ed-8fd8-c242431bdf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.7021\n",
      "Проверка весов модели...\n",
      "shared.weight torch.Size([32128, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 8])\n",
      "encoder.block.0.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.0.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.1.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.1.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.2.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.2.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.3.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.3.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.4.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.4.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "encoder.block.5.layer.0.layer_norm.weight torch.Size([512])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "encoder.block.5.layer.1.layer_norm.weight torch.Size([512])\n",
      "encoder.final_layer_norm.weight torch.Size([512])\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 8])\n",
      "decoder.block.0.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.0.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.0.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.1.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.1.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.2.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.2.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.3.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.3.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.4.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.4.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.0.layer_norm.weight torch.Size([512])\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight torch.Size([512, 512])\n",
      "decoder.block.5.layer.1.layer_norm.weight torch.Size([512])\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight torch.Size([2048, 512])\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight torch.Size([512, 2048])\n",
      "decoder.block.5.layer.2.layer_norm.weight torch.Size([512])\n",
      "decoder.final_layer_norm.weight torch.Size([512])\n",
      "Модель успешно сохранена.\n",
      "Сгенерированная предыстория: Clancy was sent on a mission to help a colleague deal with a critical situation. Rob tried to justify himself, but his words sounded unconvincing.\n"
     ]
    }
   ],
   "source": [
    "train_model('story.csv', epochs=1)\n",
    "    \n",
    "# Генерация предыстории\n",
    "\n",
    "# Здесь при выводе выводятся веса модели, для проверки того, что она правильно всё сохраняет, так как изначально модель не могла сгенерировать историю\n",
    "\n",
    "final = \"Clancy helped Rob fix the error and they returned to HQ together.\"\n",
    "print(\"Сгенерированная предыстория:\", generate_prehistory(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68c04c-3148-495d-b077-01919ae7fe82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
